{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydoc import Doc\n",
    "from chromadb.config import C\n",
    "from flask import Flask, request, jsonify\n",
    "import hashlib\n",
    "import json\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import os\n",
    "import numpy as np\n",
    "from typing import Iterator, Tuple\n",
    "import logging\n",
    "\n",
    "class DocumentStore:\n",
    "    def __init__(self, storage_path: str, chunk_size: int=1000, overlap: int=200):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            storage_path: the path to store the data\n",
    "            chunk_size: the size of the chunks to split the documents into\n",
    "            overlap: the overlap between the chunks\n",
    "        \"\"\"\n",
    "        self.storage_path = storage_path\n",
    "        self.embedded_chunks = np.array([],dtype=np.float32)\n",
    "        self.chunk_hash_to_doc_id = {}\n",
    "        self.chunk_id_to_doc_id = {}\n",
    "        self.doc_id_to_file_path = {}\n",
    "        self.pointer_start_of_doc_id = np.array([],dtype=np.int32)\n",
    "    \n",
    "        self.model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "        self.log = logging.getLogger(\"DocumentStore\")\n",
    "        self.max_doc_id = -1\n",
    "        self.max_chunk_id = -1\n",
    "\n",
    "        # for now we cannot have different chunk sizes per doc\n",
    "        self.chunk_size = chunk_size\n",
    "        self.overlap = overlap\n",
    "\n",
    "        self.load_from_disk()\n",
    "    \n",
    "    def load_from_disk(self) -> None:\n",
    "        \"\"\"\n",
    "        Loads the store from disk if it exists\n",
    "        \"\"\"\n",
    "        #storage path is the upper level directory\n",
    "        if os.path.exists(self.storage_path):\n",
    "            self.embedded_chunks = np.load(os.path.join(self.storage_path,\"embedded_chunks.npy\"))\n",
    "            self.pointer_start_of_doc_id = np.load(os.path.join(self.storage_path,\"pointer_start_of_doc_id.npy\"))\n",
    "\n",
    "            self.chunk_hash_to_doc_id = json.load(open(os.path.join(self.storage_path,\"chunk_hash_to_doc_id.json\")))\n",
    "            self.doc_id_to_file_path = json.load(open(os.path.join(self.storage_path,\"doc_id_to_file_path.json\")))\n",
    "            self.chunk_id_to_doc_id = json.load(open(os.path.join(self.storage_path,\"chunk_id_to_doc_id.json\")))\n",
    "\n",
    "            metadata = json.load(open(os.path.join(self.storage_path,\"metadata.json\")))\n",
    "            self.max_doc_id = metadata['max_doc_id']\n",
    "            self.max_chunk_id = metadata['max_chunk_id']\n",
    "            self.chunk_size = metadata['chunk_size']\n",
    "            self.overlap = metadata['overlap']\n",
    "\n",
    "            self.log.info(\"Loaded from disk\")\n",
    "        else:\n",
    "            self.log.info(\"No data found in disk\")\n",
    "        return\n",
    "    \n",
    "    def persist_to_disk(self) -> None:\n",
    "        \"\"\"\n",
    "        Writes the current state of the store to disk\n",
    "        \"\"\"\n",
    "        if not os.path.exists(self.storage_path):\n",
    "            os.makedirs(self.storage_path)\n",
    "        np.save(os.path.join(self.storage_path,\"embedded_chunks.npy\"),self.embedded_chunks)\n",
    "        json.dump(self.chunk_hash_to_doc_id,open(os.path.join(self.storage_path,\"chunk_hash_to_doc_id.json\"),\"w\"))\n",
    "        json.dump(self.doc_id_to_file_path,open(os.path.join(self.storage_path,\"doc_id_to_file_path.json\"),\"w\"))\n",
    "        json.dump(self.chunk_id_to_doc_id,open(os.path.join(self.storage_path,\"chunk_id_to_doc_id.json\"),\"w\"))\n",
    "        json.dump({'max_doc_id': self.max_doc_id, 'max_chunk_id': self.max_chunk_id, 'chunk_size': self.chunk_size, 'overlap': self.overlap},open(os.path.join(self.storage_path,\"metadata.json\"),\"w\"))\n",
    "        self.log.info(\"Persisted to disk\")\n",
    "    \n",
    "    def add_document(self, file_path: str, chunk_size: int=100, overlap: int=20) -> int:\n",
    "        \"\"\"\n",
    "        Adds a document to the store and returns the number of chunks added\n",
    "        \"\"\"\n",
    "        chunks = self.chunk_text(file_path)\n",
    "        doc_id = self.max_doc_id + 1\n",
    "        self.doc_id_to_file_path[doc_id] = file_path\n",
    "        self.pointer_start_of_doc_id = np.append(self.pointer_start_of_doc_id, self.max_chunk_id)\n",
    "\n",
    "        i=0\n",
    "        for hash,chunk  in chunks:\n",
    "            vector = self.model.encode(chunk)\n",
    "            if hash in self.chunk_hash_to_doc_id:\n",
    "                self.log.info(f\"Chunk {hash} already exists in the store, skipping...\")\n",
    "                continue\n",
    "            else : \n",
    "                i+=1\n",
    "                self.chunk_hash_to_doc_id[hash] = doc_id\n",
    "                self.max_chunk_id += 1\n",
    "                self.chunk_id_to_doc_id[self.max_chunk_id] = doc_id\n",
    "                if len(self.embedded_chunks) == 0:\n",
    "                    self.embedded_chunks = np.array(vector).reshape(-1,384)\n",
    "                else : \n",
    "                    self.embedded_chunks = np.concatenate([self.embedded_chunks,np.array(vector).reshape(-1,384)],axis=0).reshape(-1,384)\n",
    "        \n",
    "        self.log.info(f\"Added {i} chunks to the store\")\n",
    "        if i>0 : \n",
    "            self.max_doc_id += 1\n",
    "        return i\n",
    "    \n",
    "    def chunk_text(self, file_path: str) -> Iterator[Tuple[str,str]]:\n",
    "        \"\"\"\n",
    "        Returns an iterator over the chunks of the file\n",
    "        \"\"\"\n",
    "        # suboptimal because it cuts through words\n",
    "        with open(file_path, 'r', encoding='utf-8') as file:\n",
    "            prev = file.read(self.overlap)\n",
    "            while True:\n",
    "                chunk = file.read(self.chunk_size-self.overlap)\n",
    "                if not chunk:\n",
    "                    break\n",
    "                chunk = prev + chunk\n",
    "                prev = chunk[-self.overlap:]\n",
    "                yield hashlib.sha256(chunk.encode()).hexdigest(), chunk\n",
    "    \n",
    "    def get_chunk_text(self, chunk_id: int) -> str:\n",
    "        \"\"\"\n",
    "        Returns the text of the chunk with the given chunk_id\n",
    "        \"\"\"\n",
    "        doc_id = self.chunk_id_to_doc_id[chunk_id]\n",
    "        file_path = self.doc_id_to_file_path[doc_id]\n",
    "        # we know the overall chunk_id\n",
    "        # we know at which chunk_id the doc starts\n",
    "        # we know that the satrt of the i_th chunk of the doc is i*(chunk_size-overlap)\n",
    "        # chunks are 0 indexed\n",
    "        with open(file_path, 'r', encoding='utf-8') as file:\n",
    "            chunk_number_in_the_doc = chunk_id - self.pointer_start_of_doc_id[doc_id]\n",
    "            file.seek(chunk_number_in_the_doc*(self.chunk_size-self.overlap))\n",
    "            return file.read(self.chunk_size)\n",
    "\n",
    "    def similarity_search(self, query: str, k: int=5) -> list:\n",
    "        \"\"\"\n",
    "        Returns the k most similar documents to the given query\n",
    "        And the text of the chunks that are most similar to the query within the documents\n",
    "        \"\"\"\n",
    "        query_embedding = self.model.encode(query)\n",
    "        scores = np.dot(self.embedded_chunks, query_embedding)\n",
    "        top_k = np.argpartition(2-2*scores, k)[:k] # returns the topk but not in order and it's faster than sorting\n",
    "        results = []\n",
    "        for i in top_k:\n",
    "            doc_id = self.chunk_id_to_doc_id[i]\n",
    "            file_path = self.doc_id_to_file_path[doc_id]\n",
    "            results.append({'file_path': file_path, 'similarity': scores[i], 'relevant_chunk': self.get_chunk_text(i)})\n",
    "        return results\n",
    "\n",
    "global DocStore\n",
    "DocStore = DocumentStore(os.path.join(os.curdir, 'docstore'))\n",
    "app = Flask(__name__)\n",
    "\n",
    "@app.route('/process', methods=['POST'])\n",
    "def process_file():\n",
    "    chunk_cnt = DocStore.add_document(request.json.get('file_path'))\n",
    "    return jsonify({'message': 'File processed successfully', 'chunk_count': chunk_cnt})\n",
    "\n",
    "@app.route('/persist', methods=['GET'])\n",
    "def persist_to_disk():\n",
    "    DocStore.persist_to_disk()\n",
    "    return jsonify({'message': 'Documents persisted to disk'})\n",
    "\n",
    "@app.route('/load', methods=['GET'])\n",
    "def load_from_disk():\n",
    "    DocStore.load_from_disk()\n",
    "    return jsonify({'message': 'Documents loaded from disk'})\n",
    "\n",
    "@app.route('/search', methods=['POST'])\n",
    "def similarity_search():\n",
    "    query = request.json.get('query')\n",
    "    k = request.json.get('k')\n",
    "    results = DocStore.similarity_search(query, k)\n",
    "    return jsonify(results)\n",
    "\n",
    "# if __name__ == '__main__':\n",
    "#     app.run(debug=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "del DocStore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DocStore.add_document(\"/Users/hcornier/Documents/Obsidian/RAG_PLUGIN/rag_plugin/.obsidian/plugins/ai-chat-obsidian/simple_python_vector_db.py\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'file_path': '/Users/hcornier/Documents/Obsidian/RAG_PLUGIN/rag_plugin/.obsidian/plugins/ai-chat-obsidian/simple_python_vector_db.py',\n",
       "  'similarity': 0.5125543,\n",
       "  'relevant_chunk': \"d from disk'})\\n\\n@app.route('/search', methods=['POST'])\\ndef similarity_search():\\n    query = request.json.get('query')\\n    k = request.json.get('k')\\n    results = DocStore.similarity_search(query, k)\\n    return jsonify(results)\\n\\n@app.route('/get_documents', methods=['GET'])\\ndef get_documents():\\n    return jsonify(DocStore.get_documents())\\n\\n# if __name__ == '__main__':\\n#     app.run(debug=True)\\n\"}]"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DocStore.similarity_search(\"distances.append((doc, distance))\\n        distances.sort(key=lambda x: x[1])\\n        return [doc for doc, _ in distances[:k]]\\n\\n\", 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "DocStore.persist_to_disk()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
