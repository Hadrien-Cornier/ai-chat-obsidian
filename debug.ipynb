{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from pydoc import Doc\n",
    "from chromadb.config import C\n",
    "from flask import Flask, request, jsonify\n",
    "import hashlib\n",
    "import json\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import os\n",
    "import numpy as np\n",
    "from typing import Iterator, Tuple\n",
    "\n",
    "class DocumentStore:\n",
    "    def __init__(self, storage_path: str):\n",
    "        self.storage_path = storage_path\n",
    "        self.embedded_chunks = []\n",
    "        self.chunk_hash_to_doc_id = {}\n",
    "        self.doc_id_to_file_path = {}\n",
    "        self.load_from_disk()\n",
    "        self.model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "    \n",
    "    def load_from_disk(self):\n",
    "        #storage path is the upper level directory\n",
    "        if os.path.exists(self.storage_path):\n",
    "            with open(os.path.join(self.storage_path,\"embedded_chunks\"), 'r', encoding='utf-8') as f:\n",
    "                self.embedded_chunks = json.load(f)\n",
    "        else:\n",
    "            self.documents = []\n",
    "    \n",
    "    def persist_to_disk(self):\n",
    "        with open(self.storage_path, 'w', encoding='utf-8') as f:\n",
    "            json.dump(self.documents, f, ensure_ascii=False, indent=4)\n",
    "    \n",
    "    def add_document(self, file_path: str, chunk_size: int=100, overlap: int=20)-> int:\n",
    "        chunks = self.chunk_text(file_path, chunk_size, overlap)\n",
    "        embeddings = {hash : self.model.encode(chunk) for hash,chunk in chunks}\n",
    "        for hash,vector in embeddings.items():\n",
    "            self.documents = self.documents + [{\n",
    "                'file_name': os.path.basename(file_path),\n",
    "                'id': doc_hash,  # Hash of the content is the same as document_id in this simplified example\n",
    "                'embedding': list(embedding)  # Convert numpy array to list for JSON serialization\n",
    "            }]\n",
    "        return len(embeddings)\n",
    "    \n",
    "    def chunk_text(self, file_path: str, chunk_size: int=100, overlap: int=20) -> Iterator[Tuple[str,str]]:\n",
    "        # suboptimal because it cuts through words\n",
    "        with open(file_path, 'r', encoding='utf-8') as file:\n",
    "            prev = file.read(overlap)\n",
    "            while True:\n",
    "                chunk = file.read(chunk_size-overlap)\n",
    "                if not chunk:\n",
    "                    break\n",
    "                chunk = prev + chunk\n",
    "                prev = chunk[-overlap:]\n",
    "                yield hashlib.sha256(chunk.encode()).hexdigest(), chunk\n",
    "    \n",
    "    def similarity_search(self, query: str, k: int=5):\n",
    "        query_embedding = self.model.encode(query)\n",
    "        distances = []\n",
    "        for doc in self.documents:\n",
    "            # min heap can be used to optimize this\n",
    "            distance = np.linalg.norm(np.array(doc['embedding']) - np.array(query_embedding))\n",
    "            distances.append((doc, distance))\n",
    "        distances.sort(key=lambda x: x[1])\n",
    "        return [doc for doc, _ in distances[:k]]\n",
    "\n",
    "    def get_documents(self):\n",
    "        return self.documents\n",
    "\n",
    "global DocStore\n",
    "DocStore = DocumentStore('documents.json')\n",
    "app = Flask(__name__)\n",
    "\n",
    "@app.route('/process', methods=['POST'])\n",
    "def process_file():\n",
    "    chunk_cnt = DocStore.add_document(request.json.get('file_path'))\n",
    "    return jsonify({'message': 'File processed successfully', 'chunk_count': chunk_cnt})\n",
    "\n",
    "@app.route('/persist', methods=['GET'])\n",
    "def persist_to_disk():\n",
    "    DocStore.persist_to_disk()\n",
    "    return jsonify({'message': 'Documents persisted to disk'})\n",
    "\n",
    "@app.route('/load', methods=['GET'])\n",
    "def load_from_disk():\n",
    "    DocStore.load_from_disk()\n",
    "    return jsonify({'message': 'Documents loaded from disk'})\n",
    "\n",
    "@app.route('/search', methods=['POST'])\n",
    "def similarity_search():\n",
    "    query = request.json.get('query')\n",
    "    k = request.json.get('k')\n",
    "    results = DocStore.similarity_search(query, k)\n",
    "    return jsonify(results)\n",
    "\n",
    "@app.route('/get_documents', methods=['GET'])\n",
    "def get_documents():\n",
    "    return jsonify(DocStore.get_documents())\n",
    "\n",
    "# if __name__ == '__main__':\n",
    "#     app.run(debug=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "DocStore.add_document(\"/Users/hcornier/Documents/Obsidian/RAG_PLUGIN/rag_plugin/.obsidian/plugins/ai-chat-obsidian/simple_python_vector_db.py\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
